{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenatoCoronado99/Machine-Learning/blob/main/LogisticRegression_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daUk8TLr12DQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk\n",
        "import csv\n",
        " \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        " \n",
        " \n",
        "##Diccionario que tiene dividido las palabras sustantivos , verbos , adverbios para Lematizar el texto\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        " \n",
        "##Lematizar es convetir la palabra en su forma primitiva\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "##Parts of speech = partes de laspalabras que indica el significado de cada una de las palabras (pronombre , sustantivos , etc )\n",
        "from nltk import pos_tag\n",
        " \n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "##Se parar las palabras entre una lista seaprada por comas \n",
        "from nltk import word_tokenize\n",
        " \n",
        "from sklearn import model_selection,svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        " \n",
        "## Para convertir las palabras a su forma raiz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "## Metrica que sirve para depurar los terminos en los textos\n",
        "##Aquellas palabras que se repiten mas en los textos\n",
        "#TF = medida que permite extraer palabras no comunes \n",
        "##ID = frecuencia inverse .... lo que dan mas información\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6uU3rA718vJ"
      },
      "source": [
        "file= pd.read_csv(\"test.csv\",encoding='latin-1',names=[\"text\",\"value\"])\n",
        "#file.drop(['Unnamed: 2','Unnamed: 3'], axis='columns', inplace=True)\n",
        "file.drop(file.index[0:1], inplace=True)\n",
        "print(file.dtypes)\n",
        "print(file.head())\n",
        "print(file.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xhuJdfx1_1e"
      },
      "source": [
        "print(\"============================================ Seleccion de caracteristicas ===============================================\")\n",
        "##Se pone todo el texto a minuscula\n",
        "for i in file[\"text\"]:\n",
        "  file[\"text\"]=file[\"text\"].replace(i,i.lower())\n",
        "  #print(file[\"text\"])\n",
        "  ##Se para las palabras en un lista##\n",
        "file[\"text\"] = [word_tokenize(item) for item in file[\"text\"]] \n",
        "#print(file[\"text\"])\n",
        "tag_map =defaultdict(lambda:wn.NOUN)##Cada palabra q no se enceuntre que es se tomara como un sustantivo\n",
        "tag_map['J']=wn.ADJ\n",
        "tag_map['V']=wn.VERB\n",
        "tag_map['R']=wn.ADV\n",
        "##steaming = cortar las palabras \n",
        "for index, item in enumerate(file[\"text\"]):\n",
        "  f_words = []## Colocar los textos que estan limpios (palabras que no tienen signos de puntuacióón , fechas )\n",
        "  word_Lemmatized = WordNetLemmatizer() ## Lematizador = cortar las palabras con criterio de clasificar si es sustantivo, adjetivo ,verbo , etc \n",
        "  ##CLasificación de palabras\n",
        "  for word,tag in pos_tag(item):##Parts of speech = partes de laspalabras que indica el significado de cada una de las palabras (pronombre , sustantivos , etc )\n",
        "    #print(tag)\n",
        "    ##Stop word = palabras que no brindan informacióón o  tienen simbolos de puntuación\n",
        "    if word not in stopwords.words(\"english\") and word.isalpha():##palabras no son numeros y no son stop words \n",
        "      f_words.append(word_Lemmatized.lemmatize(word,tag_map[tag[0]]))  ## Ayudando al Lematizador para que clasifique las palabras dandole como parametro la primera letra del tipo de palabra\n",
        "  file.loc[index,'text_final'] = str(f_words)\n",
        "#print(file.isnull().sum())\n",
        "file = file.dropna()\n",
        "#print(file.isnull().sum())\n",
        "#print(file.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKo7XNbv1_xl"
      },
      "source": [
        "Xtrain,Xtest,ytrain,ytest=model_selection.train_test_split(file[\"text_final\"],file['value'],test_size=0.3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP6N3FvT2p5x"
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(max_features= 50000)\n",
        "tfidf_vector.fit(file[\"text_final\"].astype(str))\n",
        "Xtrain_tfidf=tfidf_vector.transform(Xtrain)\n",
        "Xtest_tfidf=tfidf_vector.transform(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oiv5hFC347M"
      },
      "source": [
        "enc=LabelEncoder()\n",
        "train_y=enc.fit_transform(ytrain)\n",
        "test_y = enc.fit_transform(ytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqaLw2vh3e6p"
      },
      "source": [
        "  \n",
        "  print(\"============================================ GridSearch CV ===============================================\")\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  p=0\n",
        "  model_params={\n",
        "      \"svm\":{\n",
        "          \"model\" : svm.SVC(),\n",
        "          \"params\": {\n",
        "              \"C\":[1,10,100],\n",
        "              \"kernel\":[\"rbf\",\"linear\"],\n",
        "              \"gamma\": [0.001,0.0001]\n",
        "          }\n",
        "      },\n",
        "      \"logistic_regression\":{\n",
        "          \"model\": LogisticRegression(),\n",
        "          \"params\":{\n",
        "              \"C\":[1,10,100],\n",
        "              \"penalty\":[\"l1\",\"l2\"]\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "  scores= []\n",
        "  for model_name,mp in model_params.items():\n",
        "    p=p+1\n",
        "    print( str(mp[\"params\"])+\"==========>\"+str(p))\n",
        "    clf = GridSearchCV(mp[\"model\"],mp[\"params\"],cv=5,n_jobs=-1)\n",
        "    clf.fit(Xtrain_tfidf,train_y)\n",
        "    scores.append({\n",
        "        \"model\":model_name,\n",
        "        \"best_score\":clf.best_score_,\n",
        "        \"best_params\":clf.best_params_,\n",
        "        \"best_estimator\":clf.best_estimator_\n",
        "    })\n",
        "  print(\"================================ Comparing_SVM_RLogistic ========================\")\n",
        "  df = pd.DataFrame(scores,columns=[\"model\",\"best_score\",\"best_params\"])\n",
        "  print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUtT3R-2aVEC"
      },
      "source": [
        "\n",
        "best_params_svm= df.iat[0,2]\n",
        "best_params_logistic_regression= df.iat[1,2]\n",
        "print(best_params_svm)\n",
        "print(best_params_logistic_regression)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sYfHMqutDX6"
      },
      "source": [
        "#print(svmc.predict(Xtest_tfidf))\n",
        "#print(Xtest_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT8x_uM9d3an"
      },
      "source": [
        "tabla = []\n",
        "print(\"====================== SVM ==============================\")\n",
        "\n",
        "#print(Xtrain_tfidf)\n",
        "enc=LabelEncoder()\n",
        "\n",
        "svmc=svm.SVC(C=best_params_svm[\"C\"],kernel=best_params_svm[\"kernel\"],gamma=best_params_svm[\"gamma\"])\n",
        "svmc.fit(Xtrain_tfidf,train_y)\n",
        "pred = svmc.predict(Xtest_tfidf)\n",
        "print(pred)\n",
        "print(\"#####################Accurary_score##################\")\n",
        "acc = accuracy_score(pred,test_y)\n",
        "print(acc*100)\n",
        "print(\"#####################Matrix de Confusión##################\")\n",
        "cm = confusion_matrix(pred,test_y)\n",
        "print(cm)\n",
        "print(\"##################### F1 Score ##################\")\n",
        "f1 = f1_score(pred,test_y)\n",
        "print(\"f1 Score -> \", f1)\n",
        "print(\"##################### RECALL ##################\")\n",
        "recall  = recall_score(pred,test_y)\n",
        "print(\"El modelo solo es capaz de identificar el %0.5f \" %recall)\n",
        "\n",
        "tabla.append({\n",
        "    \"acc\": acc,\n",
        "    \"conf_mat\" : cm,\n",
        "    \"F1\" : f1,\n",
        "    \"recall\":recall\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gozm0Yt1fWUL"
      },
      "source": [
        "print(\"====================== Logistic Regression ==============================\")\n",
        "\n",
        "r_logs= LogisticRegression(C=best_params_logistic_regression[\"C\"],penalty=best_params_logistic_regression[\"penalty\"])\n",
        "r_logs= LogisticRegression(C=1.0,penalty=\"l2\")\n",
        "r_logs.fit(Xtrain_tfidf,train_y)\n",
        "y_pred_rlogs = r_logs.predict(Xtest_tfidf)\n",
        "print(y_pred_rlogs)\n",
        "print(\"#####################Accurary_score##################\")\n",
        "acc = accuracy_score(y_pred_rlogs,test_y)\n",
        "print(acc)\n",
        "print(\"#####################Matrix de Confusión##################\")\n",
        "cm = confusion_matrix(y_pred_rlogs,test_y)\n",
        "print(cm)\n",
        "print(\"##################### F1 Score ##################\")\n",
        "f1 = f1_score(y_pred_rlogs,test_y)\n",
        "print(\"f1 Score -> \", f1)\n",
        "print(\"##################### RECALL ##################\")\n",
        "recall  = recall_score(y_pred_rlogs,test_y)\n",
        "print(\"El modelo solo es capaz de identificar el %0.5f \" %recall)\n",
        "tabla.append({\n",
        "    \"acc\": acc,\n",
        "    \"conf_mat\" : cm,\n",
        "    \"F1\" : f1,\n",
        "    \"recall\":recall\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqcQyO5gmJJU"
      },
      "source": [
        "table_dataframe = pd.DataFrame(tabla,columns=[\"acc\",\"F1\",\"recall\"])\n",
        "print(table_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}