# -*- coding: utf-8 -*-
"""ML - Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y93n4qzmMdwbBZzmhERbZzPrVOFZYuGi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import *
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as sfs
import seaborn as sns

data = pd.read_csv('lung-cancer.data')
data.replace('?', np.nan, inplace=True)
#Verificamos cuáles columnas tienen Nan
data.isnull().sum()

data['f4'].fillna(data['f4'].mode()[0], inplace=True)
data['f38'].fillna(data['f4'].mode()[0], inplace=True)
print(data)

def plot_corr(dataset):
  correlation=dataset.corr()
  plt.figure(figsize=(15,10))
  sns.heatmap(correlation,annot=True,cmap='coolwarm')
def funcion_lasso(X,y,dataset):
  lass = LassoCV()
  lass.fit(X,y)
  coeficientes = pd.Series(lass.coef_,index=X.columns)
  print(coeficientes)
  coefsort = coeficientes.sort_values()
  plt.rcParams['figure.figsize'] == (8.0,10.0)
  coefsort.plot(kind='barh')
  plt.title("features")



data1 = data

y = data['target']
X = data1.drop(['target'], axis="columns")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)
y_prob = log_reg.predict_proba(X_test)

score = log_reg.score(X_test, y_test)
print('Exactitud de data de entrenamiento: {:.3f}'.format(log_reg.score(X_train, y_train)))
print("Predicción train -> ", y_pred)
print("Score -> ", score)

#Cross Validation
log_reg1 = LogisticRegression(max_iter = 500)
accuracies = cross_val_score(log_reg1, X, y, cv=10)
print("Precisión Cross validation : %0.2f (+/- % 0.2f)" % (accuracies.mean(),accuracies.std()*2))

#Matriz de confusión
cm = confusion_matrix(y_test, y_pred)
print(cm)

funcion_lasso(X, y, X)

listDrop = ['target','f1', 'f3', 'f4', 'f5', 'f7', 'f8', 'f9', 'f11', 'f12', 'f15', 'f16', 'f17', 'f18', 'f21', 'f22', 'f24', 'f25', 'f26', 'f28', 'f29', 'f30', 'f31', 'f32', 'f35', 'f36','f38','f39','f40','f41','f42','f43','f44','f45','f46','f47','f48','f49','f50','f51','f52','f53','f54','f55','f56']
data1 = data

y = data['target']
X = data1.drop(listDrop, axis="columns")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)
y_prob = log_reg.predict_proba(X_test)

score = log_reg.score(X_test, y_test)
print('Exactitud de data de entrenamiento: {:.3f}'.format(log_reg.score(X_train, y_train)))
print("Predicción train -> ", y_pred)
print("Score -> ", score)



#Cross Validation
log_reg1 = LogisticRegression(max_iter = 500)
accuracies = cross_val_score(log_reg1, X, y, cv=10)
print("Precisión Cross validation : %0.2f (+/- % 0.2f)" % (accuracies.mean(),accuracies.std()*2))

plot_corr(X)